{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7af657e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pkg_resources\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from scipy import constants\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def coalescence_time(f, chirp_mass):\n",
    "    'Coalescence time of BBH merger'\n",
    "    return 5 * (8 * np.pi * f)**(-8/3) * chirp_mass**(-5/3)\n",
    "\n",
    "def freq(t, tc, chirp_mass):\n",
    "    'finding frequency as function of time'\n",
    "    return (((tc - t)/5)**(-3/8)) / (8*np.pi*(chirp_mass**(5/8)))\n",
    "\n",
    "def phase(tc, t, ch_mass, phi_c):   #what is this called?????\n",
    "    return phi_c - 2 * ((tc - t)/(5 * ch_mass))**(5/8)\n",
    "\n",
    "def phi_t_Lbar(time, phi_0):\n",
    "    'LISA orbital phase'\n",
    "    return phi_0 + (2*np.pi*time/constants.year)\n",
    "\n",
    "def theta_s_t(theta_sbar, phi_t_Lbar, phi_sbar):\n",
    "    'Source location'\n",
    "    cos_theta = (0.5 * np.cos(theta_sbar)) - (np.sqrt(3)/2)*np.sin(theta_sbar)*np.cos(phi_t_Lbar-phi_sbar)\n",
    "    return np.arccos(cos_theta)\n",
    "\n",
    "def alpha_i_t(i, t, alpha_0):\n",
    "    'LISA Arm orientation'\n",
    "    T = constants.year\n",
    "    return 2*np.pi*t/T - np.pi/12 - (i-1)*np.pi/3 + alpha_0\n",
    "\n",
    "def phi_s_t(theta_sbar, phi_t_bar, phi_sbar, alpha1):\n",
    "    'Source location in (unbarred) detector frame'\n",
    "    return alpha1 + np.pi/12 + np.arctan((np.sqrt(3)*np.cos(theta_sbar) + np.sin(theta_sbar)*np.cos(phi_t_bar - phi_sbar))                                   /(2*np.sin(theta_sbar)*np.sin(phi_t_bar - phi_sbar)) )\n",
    "\n",
    "def psi_s_t(theta_Lbar, phi_Lbar, theta_sbar, phi_sbar, phi_t_Lbar, theta_s_t_):\n",
    "    'Polarisation angle'\n",
    "    L_dot_z = 0.5 * np.cos(theta_Lbar) - ( (np.sqrt(3)/2) * np.sin(theta_Lbar) * np.cos(phi_t_Lbar - phi_Lbar) )\n",
    "    L_dot_n = np.cos(theta_Lbar)*np.cos(theta_sbar) + np.sin(theta_Lbar)*np.sin(theta_sbar)*np.cos(phi_Lbar - phi_sbar)\n",
    "    global cos_i\n",
    "    cos_i = L_dot_n\n",
    "    \n",
    "    cross = (0.5*np.sin(theta_Lbar)*np.sin(theta_sbar)*np.sin(phi_Lbar - phi_sbar)) - (np.sqrt(3)/2)*np.cos(phi_t_Lbar)*( (np.cos(theta_Lbar)*np.sin(theta_sbar)*np.sin(phi_sbar) -                                       np.cos(theta_sbar)*np.sin(theta_Lbar)*np.sin(phi_Lbar)) )    - (np.sqrt(3)/2)*np.sin(phi_t_Lbar)*(np.cos(theta_sbar)*np.sin(theta_Lbar)*np.cos(phi_Lbar) -                                       np.cos(theta_Lbar)*np.sin(theta_sbar)*np.cos(phi_sbar))\n",
    "    \n",
    "    tan_psi = (L_dot_z - L_dot_n * np.cos(theta_s_t_)) / cross\n",
    "    \n",
    "    return np.arctan(tan_psi)\n",
    "\n",
    "def doppler_phase(f, theta_sbar, phi, phi_sbar):\n",
    "    'doppler phase due to LISA motion'\n",
    "    R = constants.astronomical_unit/constants.c\n",
    "    return 2 * np.pi * f * R * np.sin(theta_sbar) * np.cos(phi - phi_sbar)\n",
    "\n",
    "def F_plus(theta_s, phi_s, psi_s):\n",
    "    'Detector Beam Pattern Coefficient'\n",
    "    return (0.5 * (1 + np.cos(theta_s)**2) * np.cos(2*phi_s) * np.cos(2*psi_s)) - (np.cos(theta_s) * np.sin(2*phi_s) * np.sin(2*psi_s) )\n",
    "\n",
    "def F_cross(theta_s, phi_s, psi_s):\n",
    "    'Detector Beam Pattern Coefficient'\n",
    "    return (0.5 * (1+np.cos(theta_s)**2) * np.cos(2*phi_s) * np.sin(2*psi_s)) + (np.cos(theta_s) * np.sin(2*phi_s) * np.cos(2*psi_s))\n",
    "\n",
    "def phi_P_I_t(cos_i, F_plus, F_cross): \n",
    "    'Polarisation Phase'\n",
    "    return np.arctan2( (2*cos_i*F_cross), ((1 + (cos_i**2))*F_plus) )\n",
    "\n",
    "def A_t(M_c, f, D_L):\n",
    "    'Waveform Amplitude'\n",
    "    return 2 * M_c**(5/3) * (np.pi*f)**(2/3) / D_L\n",
    "\n",
    "def A_p_t(F_plus, F_cross, cos_i):\n",
    "    'Polarization Amplitude'\n",
    "    return np.sqrt(3)/2 * (((1+cos_i**2)**2 * F_plus**2) + (4 * cos_i**2 * F_cross**2))**(1/2) \n",
    "\n",
    "def h_t(A_t, A_p_t, phase, phi_P_I_t, doppler_phase):\n",
    "    'Strain signal'\n",
    "    return A_t * A_p_t * np.cos(phase + phi_P_I_t + doppler_phase)\n",
    "\n",
    "def phi_f(f, phi_c, chirp_mass, tc):\n",
    "    'phase as func of freq'\n",
    "    return 2*np.pi*f*tc - phi_c - np.pi/4 + (3/4)*(8*np.pi*chirp_mass*f)**(-5/3)\n",
    "\n",
    "def fft(A_p_t, f, chirp_mass, D_L, phi_f, phi_p_t, phi_d_t):\n",
    "    return np.sqrt(5/96) * np.pi**(-2/3) * (1/D_L) * A_p_t * chirp_mass**(5/6) * f**(-7/6) * np.exp(1j * (phi_f - phi_p_t - phi_d_t))\n",
    "\n",
    "def P_oms(f):\n",
    "    '''Single link optical metrology noise'''\n",
    "    return (1.5e-11)**2 * (1 + (2e-3/f)**4)\n",
    "\n",
    "def P_acc(f):\n",
    "    '''Single test mass acceleration noise'''\n",
    "    return (3e-15)**2 * (1 + (0.4e-3/f)**2) * (1 + (f/8e-3)**4)\n",
    "\n",
    "def P_n(f, fstar=19.09e-3, L=2.5e9):\n",
    "    '''Total noise'''\n",
    "    return P_oms(f)/L**2 + 2*(1+(np.cos(f/fstar))**2) * P_acc(f)/((2*np.pi*f)**4 * L**2)\n",
    "\n",
    "def S_c(f):\n",
    "    '''Confusion Noise'''\n",
    "    A = 9e-45\n",
    "    alpha = 0.171\n",
    "    beta = 292\n",
    "    kappa = 1020\n",
    "    gamma = 1680\n",
    "    f_k = 0.00215\n",
    "    \n",
    "    return A * f ** (-7/3) * np.exp(-(f**alpha) + beta * f * np.sin(kappa*f)) * (1 + np.tanh(gamma*(f_k-f)))\n",
    "\n",
    "def S_n(f, fstar=19.09e-3):\n",
    "    '''Lisa Sensitivity'''\n",
    "    return S_c(f) + 10/(3*2.5e9**2) * (P_oms(f) + (4*P_acc(f)/(2*np.pi*f)**4))         * (1+0.6*(f / fstar)**2)\n",
    "        \n",
    "def noise_f(f):\n",
    "    '''Random gaussian noise with random phase between 0 2pi'''\n",
    "    phase = np.exp(1j*np.random.uniform(0, 2*np.pi, (1,200)))\n",
    "    noise = np.random.normal(0, 1) * phase\n",
    "    return noise\n",
    "\n",
    "def h_func_f2(angles, noise_ind, Nsamples=200):\n",
    "    '''Input params are 2 angles; cos_theta and phi.\n",
    "    ------------\n",
    "     Returns whitened strain signal with noise based on LISA sensitivity'''\n",
    "    theta_Lbar = np.pi/5\n",
    "    phi_Lbar = np.pi/11\n",
    "    theta_sbar = np.arccos(angles[0])\n",
    "    phi_sbar = angles[1]\n",
    "    alpha_0 = 0\n",
    "    phi_0 = 0\n",
    "    phi_c = 0\n",
    "    det_no = 1\n",
    "    fmin = 1e-4\n",
    "    D_L = constants.parsec * 1e9 / constants.c #luminosity distance in seconds (1Gpc).\n",
    "\n",
    "    #calculating coalescence time, f_isco and t_isco\n",
    "    Nmax = 200\n",
    "    delta_t = 500\n",
    "    tc = Nmax*delta_t\n",
    "    M_c = (5*(8*np.pi*fmin)**(-8/3) * tc**(-1))**(3/5)\n",
    "    M = 4**(3/5) * M_c\n",
    "    f_isco = 1 / (np.pi * 6**(3/2) * 2**(6/5) *M_c)\n",
    "\n",
    "    #time and frequency arrays\n",
    "    f = np.linspace(fmin, f_isco, Nsamples)\n",
    "    t = tc - 5 * (8*np.pi*f)**(-8/3) * M_c**(-5/3)\n",
    "    \n",
    "    #params\n",
    "    phi = phase(tc, t, M_c, phi_c)\n",
    "    phi_f_t = phi_f(f, phi_c, M_c, tc)\n",
    "    phi_t_Lbar_ = phi_t_Lbar(t, phi_0)\n",
    "    theta_s_t_ = theta_s_t(theta_sbar, phi_t_Lbar_, phi_sbar)\n",
    "    alpha_t_ = alpha_i_t(det_no, t, alpha_0)\n",
    "    phi_s_t_ = phi_s_t(theta_sbar, phi_t_Lbar_, phi_sbar, alpha_t_)\n",
    "    psi_s_t_ = psi_s_t(theta_Lbar, phi_Lbar, theta_sbar, phi_sbar, phi_t_Lbar_, theta_s_t_)\n",
    "    doppler_phase_ = doppler_phase(f, theta_sbar, phi_t_Lbar_, phi_sbar)\n",
    "    F_plus_ = F_plus(theta_s_t_, phi_s_t_, psi_s_t_)\n",
    "    F_cross_ = F_cross(theta_s_t_, phi_s_t_, psi_s_t_)\n",
    "    phi_P_I_t_ = phi_P_I_t(cos_i, F_plus_, F_cross_)\n",
    "    A_t_ = A_t(M_c, f, D_L)\n",
    "    A_p_t_ = A_p_t(F_plus_, F_cross_, cos_i)\n",
    "\n",
    "    fourier_signal = fft(A_p_t_, f, M_c, D_L, phi_f_t, phi_P_I_t_, doppler_phase_)\n",
    "    #noise equation compared to joels is slightly different but negligible. \n",
    "    return fourier_signal/np.sqrt(S_n(f)) + noise_ind * noise_f(f)\n",
    "\n",
    "def setgeometry(q):\n",
    "    global qdim, xmin, xmax, xstops, xmid, xwid\n",
    "\n",
    "    # bins\n",
    "    qdim = q\n",
    "\n",
    "    # prior range for x (will be uniform)\n",
    "    xmin, xmax = 0, 1\n",
    "\n",
    "    # definition of quantization bins\n",
    "    xstops = np.linspace(xmin, xmax, qdim + 1)\n",
    "\n",
    "    # to plot histograms\n",
    "    xmid = 0.5 * (xstops[:-1] + xstops[1:])\n",
    "    xwid = xstops[1] - xstops[0]\n",
    "\n",
    "setgeometry(64)\n",
    "\n",
    "def numpy2cuda(array, single=True):\n",
    "  array = torch.from_numpy(array)\n",
    "  \n",
    "  if single:\n",
    "    array = array.float()\n",
    "    \n",
    "  if torch.cuda.is_available():\n",
    "    array = array.cuda()\n",
    "    \n",
    "  return array\n",
    "\n",
    "\n",
    "def cuda2numpy(tensor):\n",
    "  return tensor.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def makenet(dims, softmax=True, single=True):\n",
    "  \"\"\"Make a fully connected DNN with layer widths described by `dims`.\n",
    "  CUDA is always enabled, and double precision is set with `single=False`.\n",
    "  The output layer applies a softmax transformation,\n",
    "  disabled by setting `softmax=False`.\"\"\"\n",
    "\n",
    "  ndims = len(dims)\n",
    "\n",
    "  class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # the weights must be set explicitly as attributes in the class\n",
    "      # (i.e., we can't collect them in a single list)\n",
    "      for l in range(ndims - 1):\n",
    "        layer = nn.Linear(dims[l], dims[l+1])\n",
    "        \n",
    "        if not single:\n",
    "          layer = layer.double()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "          layer = layer.cuda()\n",
    "        \n",
    "        setattr(self, f'fc{l}', layer)\n",
    "                \n",
    "    def forward(self, x):\n",
    "      # per Alvin's recipe, apply relu everywhere but last layer\n",
    "      for l in range(ndims - 2):\n",
    "        x = F.leaky_relu(getattr(self, f'fc{l}')(x), negative_slope=0.2)\n",
    "\n",
    "      x = getattr(self, f'fc{ndims - 2}')(x)\n",
    "\n",
    "      if softmax:\n",
    "        return F.softmax(x, dim=1)\n",
    "      else:\n",
    "        return x\n",
    "  \n",
    "  return Net\n",
    "\n",
    "\n",
    "def makenetbn(dims, softmax=True, single=True):\n",
    "  \"\"\"A batch-normalizing version of makenet. Experimental.\"\"\"\n",
    "\n",
    "  ndims = len(dims)\n",
    "\n",
    "  class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # the weights must be set explicitly as attributes in the class\n",
    "      # (i.e., we can't collect them in a single list)\n",
    "      for l in range(ndims - 1):\n",
    "        layer = nn.Linear(dims[l], dims[l+1])\n",
    "        bn = nn.BatchNorm1d(num_features=dims[l+1])\n",
    "        \n",
    "        if not single:\n",
    "          layer = layer.double()\n",
    "          bn = bn.double()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "          layer = layer.cuda()\n",
    "          bn = bn.cuda()\n",
    "        \n",
    "        setattr(self, f'fc{l}', layer)\n",
    "        setattr(self, f'bn{l}', bn)\n",
    "                \n",
    "    def forward(self, x):\n",
    "      # per Alvin's recipe, apply relu everywhere but last layer\n",
    "      for l in range(ndims - 2):\n",
    "        x = getattr(self, f'bn{l}')(F.leaky_relu(getattr(self, f'fc{l}')(x), negative_slope=0.2))\n",
    "\n",
    "      x = getattr(self, f'fc{ndims - 2}')(x)\n",
    "\n",
    "      if softmax:\n",
    "        return F.softmax(x, dim=1)\n",
    "      else:\n",
    "        return x\n",
    "  \n",
    "  return Net\n",
    "\n",
    "def kllossGn2(o, l: 'xtrue'):\n",
    "  \"\"\"KL loss for Gaussian-mixture output, 2D, precision-matrix parameters.\"\"\"\n",
    "\n",
    "  dx  = o[:,0::6] - l[:,0,np.newaxis]\n",
    "  dy  = o[:,2::6] - l[:,1,np.newaxis]\n",
    "  \n",
    "  # precision matrix is positive definite, so has positive diagonal terms\n",
    "  Fxx = o[:,1::6]**2\n",
    "  Fyy = o[:,3::6]**2\n",
    "  \n",
    "  # precision matrix is positive definite, so has positive \n",
    "  Fxy = torch.atan(o[:,4::6]) / (0.5*math.pi) * o[:,1::6] * o[:,3::6]\n",
    "  \n",
    "  weight = torch.softmax(o[:,5::6], dim=1)\n",
    "   \n",
    "  # omitting the sqrt(4*math*pi) since it's common to all templates\n",
    "  return -torch.mean(torch.logsumexp(torch.log(weight) - 0.5*(Fxx*dx*dx + Fyy*dy*dy + 2*Fxy*dx*dy) + 0.5*torch.log(Fxx*Fyy - Fxy*Fxy), dim=1))\n",
    "\n",
    "cos_theta_min = 0.4\n",
    "cos_theta_max = 0.6\n",
    "phi_min = 7*np.pi/4\n",
    "phi_max = 3*np.pi/2\n",
    "\n",
    "def syntrain(size,  region=[[cos_theta_min, cos_theta_max], [phi_min, phi_max]], varx='theta_N', \n",
    "             varall=True, seed=None, single=True, noise=1):\n",
    "    \"\"\"Makes a training set using the ROMAN NN. It returns labels (for `varx`,\n",
    "        or for all if `varall=True`), indicator vectors, and ROM coefficients\n",
    "        (with `snr` and `noise`). Note that the coefficients are kept on the GPU.\n",
    "        Parameters are sampled randomly within `region`.\"\"\"\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        xs = torch.zeros((size,2), dtype=torch.float, device=device)\n",
    "\n",
    "        for i, r in enumerate(region):\n",
    "            xs[:,i] = r[0] + (r[1] - r[0]) * torch.rand((size,), dtype=torch.float, device=device)\n",
    "        \n",
    "        xs_1 = xs.detach().cpu().double().numpy()\n",
    "        \n",
    "        #generating signals\n",
    "        signal = np.apply_along_axis(h_func_f2, 1, xs_1, noise)[:,:,0]\n",
    "            \n",
    "        signal_r, signal_i = numpy2cuda(signal.real), numpy2cuda(signal.imag)\n",
    "        \n",
    "        #setting up real and imag alphas\n",
    "        alphas = torch.zeros((size, 200*2), dtype=torch.float if single else torch.double, device=device)\n",
    "        \n",
    "        alphas[:,0::2] = signal_r \n",
    "        alphas[:,1::2] = signal_i \n",
    "\n",
    "    xr = xs.detach().cpu().double().numpy()\n",
    "\n",
    "    del xs, signal_r, signal_i\n",
    "\n",
    "      # normalize (for provided regions)\n",
    "    for i, r in enumerate(region):\n",
    "        xr[:,i] = (xr[:,i] - r[0]) / (r[1] - r[0])\n",
    "\n",
    "    if isinstance(varx, list):\n",
    "        ix = ['theta_N','phi'].index(varx[0])\n",
    "        jx = ['theta_N','phi'].index(varx[1])    \n",
    "\n",
    "        i = np.digitize(xr[:,ix], xstops, False) - 1\n",
    "        i[i == -1] = 0; i[i == qdim] = qdim - 1\n",
    "        px = np.zeros((size, qdim), 'd'); px[range(size), i] = 1\n",
    "\n",
    "        j = np.digitize(xr[:,jx], xstops, False) - 1\n",
    "        j[j == -1] = 0; j[j == qdim] = qdim - 1\n",
    "        py = np.zeros((size, qdim), 'd'); py[range(size), j] = 1\n",
    "\n",
    "        if varall:\n",
    "            return xr, np.einsum('ij,ik->ijk', px, py), alphas\n",
    "        else:\n",
    "            return xr[:,[ix,jx]], np.einsum('ij,ik->ijk', px, py), alphas    \n",
    "    else:\n",
    "        ix = ['theta_N','phi'].index(varx)\n",
    "\n",
    "        i = np.digitize(xr[:,ix], xstops, False) - 1\n",
    "        i[i == -1] = 0; i[i == qdim] = qdim - 1\n",
    "        px = np.zeros((size, qdim), 'd'); px[range(size), i] = 1\n",
    "\n",
    "        if varall:\n",
    "            return xr, px, alphas\n",
    "        else:\n",
    "            return xr[:,ix], px, alphas\n",
    "        \n",
    "def syntrainer(net, syntrain, lossfunction=None, iterations=300, \n",
    "               batchsize=None, initstep=1e-3, finalv=1e-5, clipgradient=None, validation=None,\n",
    "               seed=None, single=True):\n",
    "  \"\"\"Trains network NN against training sets obtained from `syntrain`,\n",
    "  iterating at most `iterations`; stops if the derivative of loss\n",
    "  (averaged over 20 epochs) becomes less than `finalv`.\"\"\"\n",
    "\n",
    "  if seed is not None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "  indicatorloss = 'l' in lossfunction.__annotations__ and lossfunction.__annotations__['l'] == 'indicator'  \n",
    "  \n",
    "  if validation is not None:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    vlabels = numpy2cuda(validation[1] if indicatorloss else validation[0], single)\n",
    "    vinputs = numpy2cuda(validation[2], single)\n",
    "  \n",
    "  optimizer = optim.Adam(net.parameters(), lr=initstep)\n",
    "\n",
    "  training_loss, validation_loss = [], []\n",
    "  \n",
    "  for epoch in range(iterations):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xtrue, indicator, inputs = syntrain()\n",
    "    labels = numpy2cuda(indicator if indicatorloss else xtrue, single)\n",
    "\n",
    "    if batchsize is None:\n",
    "      batchsize = inputs.shape[0]\n",
    "    batches = inputs.shape[0] // batchsize\n",
    "\n",
    "    averaged_loss = 0.0    \n",
    "    \n",
    "    for i in range(batches):\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = net(inputs[i*batchsize:(i+1)*batchsize])\n",
    "      loss = lossfunction(outputs, labels[i*batchsize:(i+1)*batchsize])\n",
    "      loss.backward()\n",
    "      \n",
    "      if clipgradient is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), clipgradient)\n",
    "      \n",
    "      optimizer.step()\n",
    "\n",
    "      # print statistics\n",
    "      averaged_loss += loss.item()\n",
    "\n",
    "    training_loss.append(averaged_loss/batches)\n",
    "\n",
    "    if validation is not None:\n",
    "      loss = lossfunction(net(vinputs), vlabels)\n",
    "      validation_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "    if epoch == 1:\n",
    "      print(\"One epoch = {:.1f} seconds.\".format(time.time() - t0))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "      print(epoch,training_loss[-1],validation_loss[-1] if validation is not None else '')\n",
    "\n",
    "    try:\n",
    "      if len(training_loss) > iterations/10:\n",
    "        training_rate = np.polyfit(range(20), training_loss[-20:], deg=1)[0]\n",
    "        if training_rate < 0 and training_rate > -finalv:\n",
    "          print(f\"Terminating at epoch {epoch} because training loss stopped improving sufficiently: rate = {training_rate}\")\n",
    "          break\n",
    "\n",
    "      if len(validation_loss) > iterations/10:\n",
    "        validation_rate = np.polyfit(range(20), validation_loss[-20:], deg=1)[0]        \n",
    "        if validation_rate > 0:\n",
    "          print(f\"Terminating at epoch {epoch} because validation loss started worsening: rate = {validation_rate}\")\n",
    "          break\n",
    "    except:\n",
    "      pass\n",
    "          \n",
    "  print(\"Final\",training_loss[-1],validation_loss[-1] if validation is not None else '')\n",
    "      \n",
    "  if hasattr(net,'steps'):\n",
    "    net.steps += iterations\n",
    "  else:\n",
    "    net.steps = iterations\n",
    "    \n",
    "# dimensions = [200*2] + [1024]*8 + [1*6]\n",
    "# percival_network = makenet(dimensions, softmax=False)\n",
    "\n",
    "# network_to_use = percival_network()\n",
    "\n",
    "# ##Training data to pass through Percival network\n",
    "# training_data = lambda: syntrain(size=100000, varx='theta_N')\n",
    "\n",
    "# ##Train Percival network on above data\n",
    "# ##training the network\n",
    "# syntrainer(network_to_use, training_data, lossfunction=kllossGn2, iterations=5000,\n",
    "#            initstep=1e-4, finalv=1e-8)\n",
    "\n",
    "# PATH = \"\"\n",
    "# torch.save(network_to_use.state_dict(), PATH + '\\\\Trained-Networks\\\\theta-phi_l200-1024x8_2d_5000it.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b8e1cd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11262.64857069-12448.35641839j,  11461.45279295-12562.20064194j,\n",
       "         11776.8794704 -12568.74907949j,  12205.95495933-12462.26933849j,\n",
       "         12742.61025514-12234.21823802j,  13377.44845881-11873.64042362j,\n",
       "         14097.43274732-11367.69956961j,  14885.53172448-10702.3510048j ,\n",
       "         15720.36496632 -9863.16497884j,  16575.89748906 -8836.30702802j,\n",
       "         17421.23741454 -7609.67588746j,  18220.59571682 -6174.19009054j,\n",
       "         18933.46983096 -4525.20187608j,  19515.11316159 -2664.00155655j,\n",
       "         19917.34911581  -599.35760322j,  20089.78015208 +1650.98179547j,\n",
       "         19981.42851504 +4058.9198609j ,  19542.82504693 +6584.88795729j,\n",
       "         18728.53530874 +9177.09651868j,  17500.0782811 +11771.2990707j ,\n",
       "         15829.15286977+14291.21073489j,  13701.04282195+16649.71474547j,\n",
       "         11118.02385607+18750.96996549j,   8102.55113448+20493.49728117j,\n",
       "          4699.96485595+21774.2719773j ,    980.42163852+22493.78277547j,\n",
       "         -2960.25511004+22561.93762246j,  -7001.10602472+21904.60475192j,\n",
       "        -10998.21531622+20470.48012167j, -14788.7438193 +18237.87610531j,\n",
       "        -18196.9255708 +15220.94012296j, -21042.01136823+11474.74594246j,\n",
       "        -23147.98791101 +7098.66559813j, -24354.72506696 +2237.43699584j,\n",
       "        -24530.01743692 -2920.59930809j, -23581.80430186 -8148.50420471j,\n",
       "        -21469.69195705-13189.21782167j, -18214.78417657-17767.50837788j,\n",
       "        -13906.77046379-21605.10473761j,  -8707.24673707-24438.40710588j,\n",
       "         -2848.3643105 -26037.84144707j,   3373.87065151-26227.61262825j,\n",
       "          9611.99333831-24904.35245949j,  15485.09157365-22052.98719517j,\n",
       "         20601.85206864-17758.09879369j,  24587.52352062-12209.15413689j,\n",
       "         27113.14722178 -5698.24576131j,  27924.93518287 +1390.56792499j,\n",
       "         26871.32375368 +8600.62697086j,  23925.0542917 +15430.30208041j,\n",
       "         19197.68095025+21367.63533597j,  12944.21392171+25929.89442767j,\n",
       "          5556.18801249+28705.17977203j,  -2457.71205941+29392.61899563j,\n",
       "        -10503.28047271+27837.3284786j , -17943.28480158+24056.29493958j,\n",
       "        -24148.20016596+18251.68693878j, -28551.4056808 +10808.86949258j,\n",
       "        -30704.13666007 +2277.54645765j, -30324.9122143  -6664.06939188j,\n",
       "        -27338.0542769 -15260.47040581j, -21896.36073303-22742.11488414j,\n",
       "        -14384.02236177-28395.7697745j ,  -5397.44946555-31635.66053969j,\n",
       "          4296.29938576-32068.35053756j,  13821.48209163-29544.13443439j,\n",
       "         22271.65107856-24188.36148882j,  28797.08524986-16407.53407994j,\n",
       "         32693.40683409 -6867.19422214j,  33482.12924368 +3558.639501j  ,\n",
       "         30973.82489543+13863.63539231j,  25305.59158047+23004.95700014j,\n",
       "         16946.54125701+30009.99576716j,   6668.01873372+34083.24849604j,\n",
       "         -4521.06609355+34701.53228361j, -15469.40013609+31685.97117098j,\n",
       "        -25000.63373819+25240.81108615j, -32041.3595769 +15952.07666695j,\n",
       "        -35745.86678824 +4743.15823776j, -35603.03217938 -7210.77155004j,\n",
       "        -31511.5995601 -18602.58015891j, -23812.73813305-28135.65356739j,\n",
       "        -13273.00631003-34673.79230646j,  -1016.30304636-37381.18307006j,\n",
       "         11590.50597653-35834.96498369j,  23087.55809743-30094.93778632j,\n",
       "         32091.62777435-20719.09576792j,  37465.87394848 -8719.57700437j,\n",
       "         38468.72082906 +4539.31185062j,  34862.03146034+17491.91441262j,\n",
       "         26962.51303871+28554.97380769j,  15626.44055407+36323.10079133j,\n",
       "          2165.68699308+39751.81163371j, -11798.20048003+38303.99241549j,\n",
       "        -24526.63687785+32038.83140427j, -34381.01329959+21628.44989756j,\n",
       "        -40037.13399976 +8296.05387955j, -40668.7918941  -6320.55608467j,\n",
       "        -36074.58513897-20366.09775878j, -26728.13086342-32002.55712231j,\n",
       "        -13740.98594585-39652.15902433j,   1261.20433276-42216.31908078j,\n",
       "         16336.95315778-39239.76750463j,  29478.30749888-30994.81472746j,\n",
       "         38879.75292424-18470.3431811j ,  43190.62167182 -3262.46238425j,\n",
       "         41715.3400981 +12622.71212677j,  34531.2639968 +27030.84855683j,\n",
       "         22503.86318894+37952.15815562j,   7192.48372067+43805.42674599j,\n",
       "         -9344.88948986+43672.85618927j, -24824.61864272+37450.25899189j,\n",
       "        -37052.61319911+25887.54536403j, -44238.41535806+10509.03091356j,\n",
       "        -45262.94248534 -6580.01906997j, -39859.27277734-22977.36013527j,\n",
       "        -28676.67703477-36320.71953474j, -13213.85939319-44630.71698277j,\n",
       "          4374.00289294-46609.75613364j,  21570.96707709-41851.1858405j ,\n",
       "         35859.04732844-30924.3601594j ,  45089.11535755-15318.25064467j,\n",
       "         47809.1636922  +2753.29638017j,  43499.23190766+20660.64077335j,\n",
       "         32674.28879847+35739.50742152j,  16834.79076161+45689.32492424j,\n",
       "         -1733.01473337+48929.03046565j, -20281.95561099+44854.75522141j,\n",
       "        -36008.73031749+33956.66565088j, -46479.35644432+17772.24633667j,\n",
       "        -50010.55615018 -1322.31324571j, -45946.75699816-20455.83021196j,\n",
       "        -34786.37553737-36692.15130925j, -18132.8620112 -47482.45674276j,\n",
       "          1527.47794264-51070.01687762j,  21191.59885728-46782.55204553j,\n",
       "         37796.77786388-35162.77137977j,  48699.41987833-17911.52607951j,\n",
       "         52100.58164529 +2353.15254903j,  47349.12615588+22488.46368216j,\n",
       "         35070.6347653 +39312.77261706j,  17096.37982191+50110.26681816j,\n",
       "         -3802.07893228+53074.08342889j, -24335.63287131+47614.98247464j,\n",
       "        -41214.0688158 +34482.03448612j, -51675.41299246+15670.55737401j,\n",
       "        -53942.76449929 -5873.67263164j, -47532.40048214-26711.44749342j,\n",
       "        -33358.92120421-43458.27612488j, -13614.82575353-53336.51412785j,\n",
       "          8561.70903833-54641.10086985j,  29581.78894714-47040.11618899j,\n",
       "         45986.15298931-31656.36897637j,  55017.21832462-10910.94728567j,\n",
       "         55087.85692899+11851.36583126j,  46066.47709178+32898.04986312j,\n",
       "         29326.41101501+48720.92823155j,   7545.61142225+56624.06748116j,\n",
       "        -15715.85433183+55188.5381782j , -36594.94711314+44533.13871949j,\n",
       "        -51567.75432376+26322.42411276j, -58047.78989948 +3514.78412059j,\n",
       "        -54838.40399853-20112.8742236j , -42359.35754219-40588.45585811j,\n",
       "        -22603.99950781-54413.5689934j ,   1171.69432894-59165.21086309j,\n",
       "         24981.13850048-53926.17737311j,  44774.14556104-39466.90243108j,\n",
       "         57127.62932178-18142.20301356j,  59841.97973965 +6485.46235079j,\n",
       "         52338.54685371+30237.2334996j ,  35785.55771035+49026.19858921j,\n",
       "         12925.07913941+59562.96086363j, -12375.17773956+59936.27030522j,\n",
       "        -35773.09689001+49965.49949951j, -53197.38488501+31259.12882946j,\n",
       "        -61558.9307376  +6963.19788391j, -59303.55492732-18762.52134074j,\n",
       "        -46706.45383305-41454.40882432j, -25851.79075206-57120.24766391j,\n",
       "          -294.98434423-62945.10554772j,  25538.9730897 -57802.48287157j,\n",
       "         47120.19254818-42477.081459j  ,  60609.72113687-19554.54382621j,\n",
       "         63546.49061066 +7008.48391369j,  55301.8091742 +32563.69283519j,\n",
       "         37216.61825866+52583.90548771j,  12391.46745042+63467.34868148j,\n",
       "        -14839.57247712+63190.16571124j, -39662.84481015+51688.22544063j,\n",
       "        -57636.26587792+30895.37556634j, -65487.19681356 +4425.39432924j,\n",
       "        -61713.23547805-23051.46932367j, -46874.84131687-46630.68909901j,\n",
       "        -23522.07421948-62050.0002286j ,   4237.42582657-66463.46649773j,\n",
       "         31457.24617852-58971.90225133j,  53232.72164138-40809.96057257j,\n",
       "         65586.61180919-15150.54669181j,  66199.69048848+13444.13771739j]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_func_f2([0.45, np.pi/2], noise_ind=0) + h_func_f2([-0.45, np.pi/2], noise_ind=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d138d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
