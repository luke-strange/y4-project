{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pkg_resources\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade git+https://github.com/vallis/TrueBayes.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "##geometry.py\n",
    "def setgeometry(q):\n",
    "    global qdim, xmin, xmax, xstops, xmid, xwid\n",
    "\n",
    "    # bins\n",
    "    qdim = q\n",
    "\n",
    "    # prior range for x (will be uniform)\n",
    "    xmin, xmax = 0, 1\n",
    "\n",
    "    # definition of quantization bins\n",
    "    xstops = np.linspace(xmin, xmax, qdim + 1)\n",
    "\n",
    "    # to plot histograms\n",
    "    xmid = 0.5 * (xstops[:-1] + xstops[1:])\n",
    "    xwid = xstops[1] - xstops[0]\n",
    "\n",
    "setgeometry(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utils.py\n",
    "import torch\n",
    "\n",
    "def numpy2cuda(array, single=True):\n",
    "  array = torch.from_numpy(array)\n",
    "  \n",
    "  if single:\n",
    "    array = array.float()\n",
    "    \n",
    "  if torch.cuda.is_available():\n",
    "    array = array.cuda()\n",
    "    \n",
    "  return array\n",
    "\n",
    "\n",
    "def cuda2numpy(tensor):\n",
    "  return tensor.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##network.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# import torch.optim as optim\n",
    "\n",
    "\n",
    "def makenet(dims, softmax=True, single=True):\n",
    "  \"\"\"Make a fully connected DNN with layer widths described by `dims`.\n",
    "  CUDA is always enabled, and double precision is set with `single=False`.\n",
    "  The output layer applies a softmax transformation,\n",
    "  disabled by setting `softmax=False`.\"\"\"\n",
    "\n",
    "  ndims = len(dims)\n",
    "\n",
    "  class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # the weights must be set explicitly as attributes in the class\n",
    "      # (i.e., we can't collect them in a single list)\n",
    "      for l in range(ndims - 1):\n",
    "        layer = nn.Linear(dims[l], dims[l+1])\n",
    "        \n",
    "        if not single:\n",
    "          layer = layer.double()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "          layer = layer.cuda()\n",
    "        \n",
    "        setattr(self, f'fc{l}', layer)\n",
    "                \n",
    "    def forward(self, x):\n",
    "      # per Alvin's recipe, apply relu everywhere but last layer\n",
    "      for l in range(ndims - 2):\n",
    "        x = F.leaky_relu(getattr(self, f'fc{l}')(x), negative_slope=0.2)\n",
    "\n",
    "      x = getattr(self, f'fc{ndims - 2}')(x)\n",
    "\n",
    "      if softmax:\n",
    "        return F.softmax(x, dim=1)\n",
    "      else:\n",
    "        return x\n",
    "  \n",
    "  return Net\n",
    "\n",
    "\n",
    "def makenetbn(dims, softmax=True, single=True):\n",
    "  \"\"\"A batch-normalizing version of makenet. Experimental.\"\"\"\n",
    "\n",
    "  ndims = len(dims)\n",
    "\n",
    "  class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      # the weights must be set explicitly as attributes in the class\n",
    "      # (i.e., we can't collect them in a single list)\n",
    "      for l in range(ndims - 1):\n",
    "        layer = nn.Linear(dims[l], dims[l+1])\n",
    "        bn = nn.BatchNorm1d(num_features=dims[l+1])\n",
    "        \n",
    "        if not single:\n",
    "          layer = layer.double()\n",
    "          bn = bn.double()\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "          layer = layer.cuda()\n",
    "          bn = bn.cuda()\n",
    "        \n",
    "        setattr(self, f'fc{l}', layer)\n",
    "        setattr(self, f'bn{l}', bn)\n",
    "                \n",
    "    def forward(self, x):\n",
    "      # per Alvin's recipe, apply relu everywhere but last layer\n",
    "      for l in range(ndims - 2):\n",
    "        x = getattr(self, f'bn{l}')(F.leaky_relu(getattr(self, f'fc{l}')(x), negative_slope=0.2))\n",
    "\n",
    "      x = getattr(self, f'fc{ndims - 2}')(x)\n",
    "\n",
    "      if softmax:\n",
    "        return F.softmax(x, dim=1)\n",
    "      else:\n",
    "        return x\n",
    "  \n",
    "  return Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##loss.py\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from truebayes.geometry import qdim\n",
    "from truebayes.utils import numpy2cuda, cuda2numpy\n",
    "\n",
    "def lossfunction(o, l: 'indicator'):\n",
    "  \"\"\"MSE loss for DNN histogram output, labels represented as indicator arrays.\"\"\"\n",
    "\n",
    "  return torch.mean(torch.sum(o**2,dim=1) - 2*torch.sum(o*l,dim=1))\n",
    "\n",
    "\n",
    "def kllossfunction(o, l: 'indicator'):\n",
    "  \"\"\"KL loss for DNN histogram output, labels represented as indicator arrays.\"\"\"\n",
    "\n",
    "  return -torch.mean(2*torch.sum(torch.log(o)*l, dim=1))\n",
    "\n",
    "\n",
    "def lossG1(o, l: 'xtrue'):\n",
    "  \"\"\"MSE loss for normal-PDF output (represented as a mean/variance pair).\"\"\"\n",
    "\n",
    "  # since int N^2(x;x0,s) dx = 1/(2 sqrt(pi) s)\n",
    "  # the sqerr loss is 1/(2 sqrt(pi) s) - 2 * e^{-(x_tr - x0)^2/2 s^2} / sqrt(2 pi s^2)\n",
    "  # multiplying by 2 sqrt(pi)\n",
    "  \n",
    "  return torch.mean((1 - 2*math.sqrt(2)*torch.exp(-0.5*(l - o[:,0])**2/o[:,1]**2)) / o[:,1])\n",
    "\n",
    "\n",
    "def kllossGn(o, l: 'xtrue'):\n",
    "  \"\"\"KL loss for Gaussian-mixture output (represented as a vector of concatenated mean/variance/weight triples).\"\"\"\n",
    "\n",
    "  x0 = o[:,0::3]\n",
    "  std = o[:,1::3]\n",
    "  weight = torch.softmax(o[:,2::3], dim=1)\n",
    "\n",
    "  # numerically unstable\n",
    "  # return -torch.mean(2*torch.log(torch.sum(weight * torch.exp(-0.5*(x0 - l[:,np.newaxis])**2/std**2) / torch.sqrt(2 * math.pi * std**2),dim=1)))\n",
    "  \n",
    "  return -torch.mean(torch.logsumexp(torch.log(weight) - 0.5*(x0 - l[:,np.newaxis])**2/std**2 - 0.5*torch.log(2 * math.pi * std**2), dim=1))\n",
    "\n",
    "\n",
    "def netmeanGn(inputs, net=None, single=True):\n",
    "  if isinstance(inputs, np.ndarray):\n",
    "    inputs = numpy2cuda(inputs, single)\n",
    "    \n",
    "  pars = cuda2numpy(net(inputs))\n",
    "\n",
    "  dx  = pars[:,0::3] \n",
    "  std = pars[:,1::3]\n",
    "  pweight = torch.softmax(torch.from_numpy(pars[:,2::3]),dim=1).numpy()\n",
    "\n",
    "  # see https://en.wikipedia.org/wiki/Mixture_distribution\n",
    "  xmean = np.sum(pweight * dx, axis=1)\n",
    "  xerr  = np.sqrt(np.sum(pweight * (dx**2 + std**2), axis=1) - xmean**2)\n",
    "\n",
    "  return xmean, xerr\n",
    "\n",
    "\n",
    "def kllossfunction2(o, l: 'indicator'):\n",
    "  \"\"\"KL loss over 2-D histogram.\"\"\"\n",
    "\n",
    "  q = o.reshape((o.shape[0], qdim, qdim))\n",
    "\n",
    "  return torch.mean(-torch.sum(torch.log(q)*l, dim=[1,2]))\n",
    "\n",
    "\n",
    "def kllossGn2(o, l: 'xtrue'):\n",
    "  \"\"\"KL loss for Gaussian-mixture output, 2D, precision-matrix parameters.\"\"\"\n",
    "\n",
    "  dx  = o[:,0::6] - l[:,0,np.newaxis]\n",
    "  dy  = o[:,2::6] - l[:,1,np.newaxis]\n",
    "  \n",
    "  # precision matrix is positive definite, so has positive diagonal terms\n",
    "  Fxx = o[:,1::6]**2\n",
    "  Fyy = o[:,3::6]**2\n",
    "  \n",
    "  # precision matrix is positive definite, so has positive \n",
    "  Fxy = torch.atan(o[:,4::6]) / (0.5*math.pi) * o[:,1::6] * o[:,3::6]\n",
    "  \n",
    "  weight = torch.softmax(o[:,5::6], dim=1)\n",
    "   \n",
    "  # omitting the sqrt(4*math*pi) since it's common to all templates\n",
    "  return -torch.mean(torch.logsumexp(torch.log(weight) - 0.5*(Fxx*dx*dx + Fyy*dy*dy + 2*Fxy*dx*dy) + 0.5*torch.log(Fxx*Fyy - Fxy*Fxy), dim=1))\n",
    "\n",
    "\n",
    "def netmeanGn2(inputs, net=None, single=True):\n",
    "  if isinstance(inputs, np.ndarray):\n",
    "    inputs = numpy2cuda(inputs, single)\n",
    "    \n",
    "  pars = cuda2numpy(net(inputs))\n",
    "\n",
    "  dx, dy = pars[:,0::6], pars[:,2::6] \n",
    "  \n",
    "  Fxx, Fyy = pars[:,1::6]**2, pars[:,3::6]**2\n",
    "  Fxy = np.arctan(pars[:,4::6]) / (0.5*math.pi) * pars[:,1::6] * pars[:,3::6]\n",
    "\n",
    "  det = Fxx*Fyy - Fxy*Fxy\n",
    "  Cxx, Cyy, Cxy = Fyy/det, Fxx/det, -Fxy/det\n",
    "\n",
    "  pweight = torch.softmax(torch.from_numpy(pars[:,5::6]),dim=1).numpy()\n",
    "\n",
    "  xmean, ymean = np.sum(pweight * dx, axis=1), np.sum(pweight * dy, axis=1)\n",
    "  xerr,  yerr  = np.sqrt(np.sum(pweight * (dx**2 + Cxx), axis=1) - xmean**2), np.sqrt(np.sum(pweight * (dy**2 + Cyy), axis=1) - ymean**2) \n",
    "  xycov        = np.sum(pweight * (dx*dy + Cxy), axis=1) - xmean*ymean\n",
    "\n",
    "  return np.vstack((xmean, ymean)).T, np.vstack((xerr, yerr)).T, xycov\n",
    "\n",
    "\n",
    "def sqerr(o, l: 'xtrue'):\n",
    "  \"\"\"Squared error loss for estimator output.\"\"\"\n",
    "\n",
    "  return torch.mean((o - l)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import constants\n",
    "m_sun = 1.98840987e30 * constants.G / (constants.c**3)                    #solar mass in seconds\n",
    "chirp_min = 32.9 #see notes for 4/11/21 of explanation.\n",
    "chirp_max = 42.9 #guess?\n",
    "from GWFunctions import h_func_f\n",
    "from GWNoise import noise_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.02630673e-14-5.79518299e-13j,  1.07659106e-13-5.70428824e-13j,\n",
       "        2.57213039e-13-5.19068634e-13j,  3.87264114e-13-4.29218054e-13j,\n",
       "        4.88084630e-13-3.07547446e-13j,  5.52093515e-13-1.63140299e-13j,\n",
       "        5.74443114e-13-6.82955928e-15j,  5.53406535e-13+1.49605562e-13j,\n",
       "        4.90533252e-13+2.94323597e-13j,  3.90557681e-13+4.16319745e-13j,\n",
       "        2.61063956e-13+5.06271436e-13j,  1.11928857e-13+5.57265063e-13j,\n",
       "       -4.54177577e-14+5.65346305e-13j, -1.98860830e-13+5.29848956e-13j,\n",
       "       -3.36531199e-13+4.53473430e-13j, -4.47728654e-13+3.42104996e-13j,\n",
       "       -5.23764963e-13+2.04381833e-13j, -5.58659747e-13+5.10427690e-14j,\n",
       "       -5.49632323e-13-1.05897603e-13j, -4.97347582e-13-2.54084229e-13j,\n",
       "       -4.05892578e-13-3.81795163e-13j, -2.82481403e-13-4.78876813e-13j,\n",
       "       -1.36907487e-13-5.37565801e-13j,  1.92169807e-14-5.53129956e-13j,\n",
       "        1.73376946e-13-5.24274113e-13j,  3.13154497e-13-4.53274222e-13j,\n",
       "        4.27233540e-13-3.45824493e-13j,  5.06327737e-13-2.10605284e-13j,\n",
       "        5.43954851e-13-5.86022903e-14j,  5.36991678e-13+9.77714148e-14j,\n",
       "        4.85960337e-13+2.45681737e-13j,  3.95017867e-13+3.72927540e-13j,\n",
       "        2.71644952e-13+4.68955890e-13j,  1.26054567e-13+5.25753583e-13j,\n",
       "       -2.96349511e-14+5.38538542e-13j, -1.82397201e-13+5.06189772e-13j,\n",
       "       -3.19384518e-13+4.31375251e-13j, -4.29014110e-13+3.20361728e-13j,\n",
       "       -5.01961505e-13+1.82517031e-13j, -5.31975168e-13+2.95418131e-14j,\n",
       "       -5.16439566e-13-1.25508359e-13j, -4.56633942e-13-2.69330439e-13j,\n",
       "       -3.57659078e-13-3.89517752e-13j, -2.28032482e-13-4.75642004e-13j,\n",
       "       -7.89811523e-14-5.20178349e-13j,  7.65118277e-14-5.19189997e-13j,\n",
       "        2.24827855e-13-4.72708035e-13j,  3.52905011e-13-3.84767459e-13j,\n",
       "        4.49397907e-13-2.63089925e-13j,  5.05701659e-13-1.18434822e-13j,\n",
       "        5.16746217e-13+3.63296725e-14j,  4.81485677e-13+1.87357666e-13j,\n",
       "        4.03032814e-13+3.21060867e-13j,  2.88420003e-13+4.25338602e-13j,\n",
       "        1.48001072e-13+4.90690852e-13j, -5.45829681e-15+5.11110698e-13j,\n",
       "       -1.57924388e-13+4.84670231e-13j, -2.95372888e-13+4.13740133e-13j,\n",
       "       -4.05084591e-13+3.04815506e-13j, -4.76839525e-13+1.67956363e-13j,\n",
       "       -5.03896208e-13+1.58873193e-14j, -4.83660075e-13-1.37165996e-13j,\n",
       "       -4.17971988e-13-2.76799625e-13j, -3.12981904e-13-3.89791259e-13j,\n",
       "       -1.78611299e-13-4.65369125e-13j, -2.76472586e-14-4.96266863e-13j,\n",
       "        1.25452491e-13-4.79458879e-13j,  2.65934454e-13-4.16498737e-13j,\n",
       "        3.80174812e-13-3.13419591e-13j,  4.57017836e-13-1.80197166e-13j,\n",
       "        4.88894627e-13-2.98185901e-14j,  4.72607609e-13+1.22959824e-13j,\n",
       "        4.09696212e-13+2.63049847e-13j,  3.06338470e-13+3.76525236e-13j,\n",
       "        1.72788287e-13+4.52024532e-13j,  2.23946993e-14+4.81920172e-13j,\n",
       "       -1.29707274e-13+4.63131018e-13j, -2.68108842e-13+3.97488159e-13j,\n",
       "       -3.78693335e-13+2.91606818e-13j, -4.50096746e-13+1.56266427e-13j,\n",
       "       -4.74910347e-13+5.35153969e-15j, -4.50495142e-13-1.45546879e-13j,\n",
       "       -3.79314648e-13-2.80732140e-13j, -2.68740125e-13-3.86041866e-13j,\n",
       "       -1.30336475e-13-4.50355949e-13j,  2.13078534e-14-4.66810028e-13j,\n",
       "        1.70093404e-13-4.33578655e-13j,  3.00111231e-13-3.54134272e-13j,\n",
       "        3.97355138e-13-2.36941686e-13j,  4.51259949e-13-9.46074429e-14j,\n",
       "        4.55894475e-13+5.74367003e-14j,  4.10670416e-13+2.02583752e-13j,\n",
       "        3.20477388e-13+3.24861821e-13j,  1.95214914e-13+4.10708716e-13j,\n",
       "        4.87583244e-14+4.50512640e-13j, -1.02540051e-13+4.39740820e-13j,\n",
       "       -2.41657629e-13+3.79518645e-13j, -3.52819691e-13+2.76579735e-13j,\n",
       "       -4.23313399e-13+1.42576368e-13j, -4.44987249e-13-7.18737270e-15j,\n",
       "       -4.15254928e-13-1.55468914e-13j, -3.37474079e-13-2.85060116e-13j,\n",
       "       -2.20639137e-13-3.80796568e-13j, -7.84058241e-14-4.31368338e-13j,\n",
       "        7.24555315e-14-4.30712885e-13j,  2.14009612e-13-3.78813092e-13j,\n",
       "        3.29286459e-13-2.81788514e-13j,  4.04341012e-13-1.51249208e-13j,\n",
       "        4.29990177e-13-2.96976950e-15j,  4.03006316e-13+1.44973846e-13j,\n",
       "        3.26605028e-13+2.74391186e-13j,  2.10146545e-13+3.69226713e-13j,\n",
       "        6.80650776e-14+4.17593513e-13j, -8.18623341e-14+4.13330002e-13j,\n",
       "       -2.20708489e-13+3.56869008e-13j, -3.30785280e-13+2.55288186e-13j,\n",
       "       -3.97929619e-13+1.21510633e-13j, -4.13388169e-13-2.72671417e-14j,\n",
       "       -3.75043820e-13-1.71739492e-13j, -2.87803057e-13-2.92983500e-13j,\n",
       "       -1.63066882e-13-3.74960372e-13j, -1.73264927e-14-4.06694683e-13j,\n",
       "        1.29956463e-13-3.83829755e-13j,  2.58922599e-13-3.09329478e-13j,\n",
       "        3.52005970e-13-1.93203456e-13j,  3.96379469e-13-5.12608795e-14j,\n",
       "        3.85809131e-13+9.69674058e-14j,  3.21639798e-13+2.30877899e-13j,\n",
       "        2.12744547e-13+3.31662925e-13j,  7.44094339e-14+3.85001168e-13j,\n",
       "       -7.37247195e-14+3.83181132e-13j, -2.10401261e-13+3.26332922e-13j,\n",
       "       -3.15795239e-13+2.22558446e-13j, -3.74435977e-13+8.69015011e-14j,\n",
       "       -3.77570120e-13-6.07328329e-14j, -3.24593908e-13-1.98439317e-13j,\n",
       "       -2.23305721e-13-3.05554882e-13j, -8.88963887e-14-3.65806867e-13j,\n",
       "        5.82168719e-14-3.69883952e-13j,  1.95424436e-13-3.17012335e-13j,\n",
       "        3.01389311e-13-2.15253919e-13j,  3.59417757e-13-8.04294295e-14j,\n",
       "        3.60199102e-13+6.62182598e-14j,  3.03452201e-13+2.01298769e-13j,\n",
       "        1.98168661e-13+3.02997872e-13j,  6.13550736e-14+3.54663302e-13j,\n",
       "       -8.45822427e-14+3.47658112e-13j, -2.15434055e-13+2.82977218e-13j,\n",
       "       -3.09206416e-13+1.71302993e-13j, -3.49898663e-13+3.14222540e-14j,\n",
       "       -3.30384617e-13-1.12797299e-13j, -2.53858832e-13-2.36413523e-13j,\n",
       "       -1.33529961e-13-3.17744303e-13j,  9.46568821e-15-3.42280116e-13j,\n",
       "        1.49641544e-13-3.05456667e-13j,  2.61654562e-13-2.13733024e-13j,\n",
       "        3.24939604e-13-8.36966241e-14j,  3.27635880e-13+6.07364265e-14j,\n",
       "        2.69036937e-13+1.92597834e-13j,  1.60026555e-13+2.86887025e-13j,\n",
       "        2.13136543e-14+3.25412734e-13j, -1.20307106e-13+3.00509748e-13j,\n",
       "       -2.37041009e-13+2.16842637e-13j, -3.05588621e-13+9.08351997e-14j,\n",
       "       -3.11968584e-13-5.22900896e-14j, -2.54648108e-13-1.83393237e-13j,\n",
       "       -1.45238857e-13-2.75320365e-13j, -6.47174959e-15-3.08654122e-13j,\n",
       "        1.32280278e-13-2.76082332e-13j,  2.41115907e-13-1.84362400e-13j,\n",
       "        2.96121946e-13-5.33058088e-14j,  2.84875456e-13+8.81784298e-14j,\n",
       "        2.09630912e-13+2.08279622e-13j,  8.73152173e-14+2.79452772e-13j,\n",
       "       -5.38797076e-14+2.84961709e-13j, -1.80735128e-13+2.23216105e-13j,\n",
       "       -2.62778343e-13+1.08712857e-13j, -2.79808994e-13-3.08867680e-14j,\n",
       "       -2.27300356e-13-1.61100630e-13j, -1.18160138e-13-2.49046979e-13j,\n",
       "        1.98751666e-14-2.71944782e-13j,  1.50871233e-13-2.23483579e-13j,\n",
       "        2.39910830e-13-1.16202493e-13j,  2.62609224e-13+2.10899899e-14j,\n",
       "        2.12329565e-13+1.50557403e-13j,  1.02870010e-13+2.35598733e-13j,\n",
       "       -3.45481512e-14+2.51450815e-13j, -1.59623318e-13+1.93048783e-13j,\n",
       "       -2.34641622e-13+7.75316062e-14j, -2.36210350e-13-5.98894714e-14j,\n",
       "       -1.63380280e-13-1.76059069e-13j, -3.91775581e-14-2.33358534e-13j,\n",
       "        9.56541107e-14-2.12451454e-13j,  1.95434947e-13-1.19906327e-13j,\n",
       "        2.25155299e-13+1.25263634e-14j,  1.73671915e-13+1.37700652e-13j,\n",
       "        5.93609339e-14+2.09434971e-13j, -7.48596240e-14+2.00093530e-13j,\n",
       "       -1.76594531e-13+1.12700045e-13j, -2.04444398e-13-1.80377587e-14j])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_func_f(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntrain(size,  region=[[chirp_min, chirp_max]], snrs=[8,16], varx='Mc', varall=False, seed=None,\n",
    "             single=True, noi=1):\n",
    "    \"\"\"Makes a training set using the ROMAN NN. It returns labels (for `varx`,\n",
    "        or for all if `varall=True`), indicator vectors, and ROM coefficients\n",
    "        (with `snr` and `noise`). Note that the coefficients are kept on the GPU.\n",
    "        Parameters are sampled randomly within `region`.\"\"\"\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\n",
    "    \n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        xs = torch.zeros((size, 1), dtype=torch.float, device=device)\n",
    "\n",
    "        for i, r in enumerate(region):\n",
    "            xs[:,0] = r[0] + (r[1] - r[0]) * torch.rand((size,), dtype=torch.float, device=device)\n",
    "\n",
    "        xs_1 = xs.detach().cpu().double().numpy()\n",
    "        \n",
    "        #generating signal and noise\n",
    "        signal = np.apply_along_axis(h_func_f, 1, xs_1)[:,:,0]\n",
    "        signal_r, signal_i = numpy2cuda(signal.real), numpy2cuda(signal.imag)\n",
    "        \n",
    "        #noise = np.apply_along_axis(noise_f, 1, f)[:,:,0]\n",
    "        #print(noise)\n",
    "        #noise_r, noise_i = numpy2cuda(noise.real), numpy2cuda(noise.imag)\n",
    "        \n",
    "        alphas = torch.zeros((size, 200*2), dtype=torch.float if single else torch.double, device=device)\n",
    "        \n",
    "        ##Normalize the vector basis \n",
    "        normalize1 = torch.sqrt(torch.sum(signal_r*signal_r + signal_i*signal_i, dim=1))\n",
    "        #normalize2 = torch.sqrt(torch.sum(noise_r*noise_r + noise_i*noise_i, dim=1))\n",
    "        \n",
    "        ##Add noise and normalise.\n",
    "        alphas[:,0::2] = 10 * signal_r /normalize1[:, np.newaxis] + noi* torch.randn((size,200), device=device)\n",
    "        alphas[:,1::2] = 10 * signal_i /normalize1[:, np.newaxis] + noi* torch.randn((size,200), device=device)\n",
    "\n",
    "        xr = np.zeros((size, 1), 'd')\n",
    "        xr = xs.detach().cpu().double().numpy()\n",
    "\n",
    "    del xs\n",
    "\n",
    "    for r in region:\n",
    "        xr[:,0] = (xr[:,0] - r[0]) / (r[1] - r[0])\n",
    "    \n",
    "    \n",
    "    ##i is index of the bins for each value in the input array\n",
    "    i = np.digitize(xr[:,0], xstops, False) - 1\n",
    "    i[i == -1] = 0; i[i == qdim] = qdim - 1\n",
    "    px = np.zeros((size, qdim), 'd'); px[range(size), i] = 1\n",
    "\n",
    "    if varall:\n",
    "        return xr, px, alphas\n",
    "    else:\n",
    "        return xr[:,0], px, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0001    ]\n",
      " [0.00010018]\n",
      " [0.00010036]\n",
      " [0.00010053]\n",
      " [0.00010072]\n",
      " [0.0001009 ]\n",
      " [0.00010108]\n",
      " [0.00010126]\n",
      " [0.00010145]\n",
      " [0.00010164]\n",
      " [0.00010182]\n",
      " [0.00010201]\n",
      " [0.0001022 ]\n",
      " [0.0001024 ]\n",
      " [0.00010259]\n",
      " [0.00010278]\n",
      " [0.00010298]\n",
      " [0.00010318]\n",
      " [0.00010338]\n",
      " [0.00010358]\n",
      " [0.00010378]\n",
      " [0.00010398]\n",
      " [0.00010418]\n",
      " [0.00010439]\n",
      " [0.0001046 ]\n",
      " [0.00010481]\n",
      " [0.00010502]\n",
      " [0.00010523]\n",
      " [0.00010544]\n",
      " [0.00010566]\n",
      " [0.00010587]\n",
      " [0.00010609]\n",
      " [0.00010631]\n",
      " [0.00010653]\n",
      " [0.00010676]\n",
      " [0.00010698]\n",
      " [0.00010721]\n",
      " [0.00010744]\n",
      " [0.00010767]\n",
      " [0.0001079 ]\n",
      " [0.00010814]\n",
      " [0.00010837]\n",
      " [0.00010861]\n",
      " [0.00010885]\n",
      " [0.00010909]\n",
      " [0.00010934]\n",
      " [0.00010958]\n",
      " [0.00010983]\n",
      " [0.00011008]\n",
      " [0.00011033]\n",
      " [0.00011059]\n",
      " [0.00011084]\n",
      " [0.0001111 ]\n",
      " [0.00011136]\n",
      " [0.00011162]\n",
      " [0.00011189]\n",
      " [0.00011216]\n",
      " [0.00011243]\n",
      " [0.0001127 ]\n",
      " [0.00011297]\n",
      " [0.00011325]\n",
      " [0.00011353]\n",
      " [0.00011381]\n",
      " [0.0001141 ]\n",
      " [0.00011439]\n",
      " [0.00011468]\n",
      " [0.00011497]\n",
      " [0.00011526]\n",
      " [0.00011556]\n",
      " [0.00011586]\n",
      " [0.00011617]\n",
      " [0.00011648]\n",
      " [0.00011679]\n",
      " [0.0001171 ]\n",
      " [0.00011742]\n",
      " [0.00011774]\n",
      " [0.00011806]\n",
      " [0.00011838]\n",
      " [0.00011871]\n",
      " [0.00011905]\n",
      " [0.00011938]\n",
      " [0.00011972]\n",
      " [0.00012007]\n",
      " [0.00012041]\n",
      " [0.00012076]\n",
      " [0.00012112]\n",
      " [0.00012148]\n",
      " [0.00012184]\n",
      " [0.0001222 ]\n",
      " [0.00012257]\n",
      " [0.00012295]\n",
      " [0.00012333]\n",
      " [0.00012371]\n",
      " [0.0001241 ]\n",
      " [0.00012449]\n",
      " [0.00012489]\n",
      " [0.00012529]\n",
      " [0.00012569]\n",
      " [0.0001261 ]\n",
      " [0.00012652]\n",
      " [0.00012694]\n",
      " [0.00012737]\n",
      " [0.0001278 ]\n",
      " [0.00012823]\n",
      " [0.00012868]\n",
      " [0.00012912]\n",
      " [0.00012958]\n",
      " [0.00013004]\n",
      " [0.0001305 ]\n",
      " [0.00013097]\n",
      " [0.00013145]\n",
      " [0.00013194]\n",
      " [0.00013243]\n",
      " [0.00013292]\n",
      " [0.00013343]\n",
      " [0.00013394]\n",
      " [0.00013446]\n",
      " [0.00013499]\n",
      " [0.00013552]\n",
      " [0.00013606]\n",
      " [0.00013661]\n",
      " [0.00013717]\n",
      " [0.00013774]\n",
      " [0.00013831]\n",
      " [0.0001389 ]\n",
      " [0.00013949]\n",
      " [0.00014009]\n",
      " [0.0001407 ]\n",
      " [0.00014133]\n",
      " [0.00014196]\n",
      " [0.0001426 ]\n",
      " [0.00014326]\n",
      " [0.00014392]\n",
      " [0.0001446 ]\n",
      " [0.00014529]\n",
      " [0.00014599]\n",
      " [0.0001467 ]\n",
      " [0.00014743]\n",
      " [0.00014816]\n",
      " [0.00014892]\n",
      " [0.00014969]\n",
      " [0.00015047]\n",
      " [0.00015126]\n",
      " [0.00015208]\n",
      " [0.00015291]\n",
      " [0.00015375]\n",
      " [0.00015462]\n",
      " [0.0001555 ]\n",
      " [0.0001564 ]\n",
      " [0.00015732]\n",
      " [0.00015826]\n",
      " [0.00015922]\n",
      " [0.0001602 ]\n",
      " [0.0001612 ]\n",
      " [0.00016223]\n",
      " [0.00016329]\n",
      " [0.00016436]\n",
      " [0.00016547]\n",
      " [0.0001666 ]\n",
      " [0.00016776]\n",
      " [0.00016896]\n",
      " [0.00017018]\n",
      " [0.00017144]\n",
      " [0.00017273]\n",
      " [0.00017406]\n",
      " [0.00017542]\n",
      " [0.00017683]\n",
      " [0.00017828]\n",
      " [0.00017977]\n",
      " [0.00018131]\n",
      " [0.0001829 ]\n",
      " [0.00018454]\n",
      " [0.00018624]\n",
      " [0.00018799]\n",
      " [0.00018981]\n",
      " [0.00019169]\n",
      " [0.00019365]\n",
      " [0.00019568]\n",
      " [0.00019779]\n",
      " [0.00019998]\n",
      " [0.00020227]\n",
      " [0.00020466]\n",
      " [0.00020715]\n",
      " [0.00020976]\n",
      " [0.00021249]\n",
      " [0.00021536]\n",
      " [0.00021838]\n",
      " [0.00022156]\n",
      " [0.00022491]\n",
      " [0.00022846]\n",
      " [0.00023222]\n",
      " [0.00023622]\n",
      " [0.00024048]\n",
      " [0.00024504]\n",
      " [0.00024993]\n",
      " [0.0002552 ]\n",
      " [0.00026091]\n",
      " [0.00026711]\n",
      " [0.00027389]\n",
      " [0.00028135]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-35fabe386041>\u001b[0m in \u001b[0;36msyntrain\u001b[1;34m(size, region, snrs, varx, varall, seed, single, noi)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0msignal_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignal_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy2cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy2cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mnoise\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mnoise_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnoise_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy2cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy2cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "syntrain(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def syntrain(snr=[8,12], size=100000, varx='Mc', nets=(ar, ai), seed=None, noise=1, varall=False,\n",
    "#              region=[[0.2,0.5], [0.2,0.25], [-1,1], [-1,1]], single=True):\n",
    "#   \"\"\"Makes a training set using the ROMAN NN. It returns labels (for `varx`,\n",
    "#   or for all if `varall=True`), indicator vectors, and ROM coefficients\n",
    "#   (with `snr` and `noise`). Note that the coefficients are kept on the GPU.\n",
    "#   Parameters are sampled randomly within `region`.\"\"\"\n",
    "\n",
    "#   device = 'cuda:0' if torch.cuda.is_available() else 'cpu:0'\n",
    "\n",
    "#   if seed is not None:\n",
    "#     np.random.seed(seed)\n",
    "#     torch.manual_seed(seed)\n",
    "\n",
    "#   with torch.no_grad():\n",
    "#     xs = torch.zeros((size,4), dtype=torch.float, device=device)\n",
    "    \n",
    "#     for i,r in enumerate(region):\n",
    "#       xs[:,i] = r[0] + (r[1] - r[0])*torch.rand((size,), dtype=torch.float, device=device)\n",
    "    \n",
    "#     # handle banks with reduced dimensionality \n",
    "#     for i in range(len(region),4):\n",
    "#       xs[:,i] = 0.0\n",
    "\n",
    "#     snrs = numpy2cuda(np.random.uniform(*snr,size=size))\n",
    "      \n",
    "#     alphas = torch.zeros((size, 241*2), dtype=torch.float if single else torch.double, device=device)\n",
    "\n",
    "#     alphar, alphai = nets[0](xs), nets[1](xs)\n",
    "#     norm = torch.sqrt(torch.sum(alphar*alphar + alphai*alphai,dim=1))\n",
    " \n",
    "#     alphas[:,0::2] = snrs[:,np.newaxis] * alphar / norm[:,np.newaxis] + noise * torch.randn((size,241), device=device)\n",
    "#     alphas[:,1::2] = snrs[:,np.newaxis] * alphai / norm[:,np.newaxis] + noise * torch.randn((size,241), device=device)\n",
    "  \n",
    "#   xr = np.zeros((size, 5),'d')\n",
    "#   xr[:,:4] = xs.detach().cpu().double().numpy()\n",
    "#   xr[:,4] = snrs.detach().cpu()\n",
    "  \n",
    "#   del xs, alphar, alphai, norm\n",
    "\n",
    "#   # normalize (for provided regions)\n",
    "#   for i, r in enumerate(region):\n",
    "#     xr[:,i] = (xr[:,i] - r[0]) / (r[1] - r[0])\n",
    "\n",
    "#   if isinstance(varx, list):\n",
    "#     ix = ['Mc','nu','chi1','chi2'].index(varx[0])\n",
    "#     jx = ['Mc','nu','chi1','chi2'].index(varx[1])    \n",
    "\n",
    "#     i = np.digitize(xr[:,ix], xstops, False) - 1\n",
    "#     i[i == -1] = 0; i[i == qdim] = qdim - 1\n",
    "#     px = np.zeros((size, qdim), 'd'); px[range(size), i] = 1\n",
    "\n",
    "#     j = np.digitize(xr[:,jx], xstops, False) - 1\n",
    "#     j[j == -1] = 0; j[j == qdim] = qdim - 1\n",
    "#     py = np.zeros((size, qdim), 'd'); py[range(size), j] = 1\n",
    "\n",
    "#     if varall:\n",
    "#       return xr, np.einsum('ij,ik->ijk', px, py), alphas\n",
    "#     else:\n",
    "#       return xr[:,[ix,jx]], np.einsum('ij,ik->ijk', px, py), alphas    \n",
    "#   else:\n",
    "#     ix = ['Mc','nu','chi1','chi2'].index(varx)\n",
    "  \n",
    "#     i = np.digitize(xr[:,ix], xstops, False) - 1\n",
    "#     i[i == -1] = 0; i[i == qdim] = qdim - 1\n",
    "#     px = np.zeros((size, qdim), 'd'); px[range(size), i] = 1\n",
    "  \n",
    "#     if varall:\n",
    "#       return xr, px, alphas\n",
    "#     else:\n",
    "#       return xr[:,ix], px, alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def syntrainer(net, syntrain, lossfunction=None, iterations=300, \n",
    "               batchsize=None, initstep=1e-3, finalv=1e-5, clipgradient=None, validation=None,\n",
    "               seed=None, single=True):\n",
    "  \"\"\"Trains network NN against training sets obtained from `syntrain`,\n",
    "  iterating at most `iterations`; stops if the derivative of loss\n",
    "  (averaged over 20 epochs) becomes less than `finalv`.\"\"\"\n",
    "\n",
    "  if seed is not None:\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "  indicatorloss = 'l' in lossfunction.__annotations__ and lossfunction.__annotations__['l'] == 'indicator'  \n",
    "  \n",
    "  if validation is not None:\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    vlabels = numpy2cuda(validation[1] if indicatorloss else validation[0], single)\n",
    "    vinputs = numpy2cuda(validation[2], single)\n",
    "  \n",
    "  optimizer = optim.Adam(net.parameters(), lr=initstep)\n",
    "\n",
    "  training_loss, validation_loss = [], []\n",
    "  \n",
    "  for epoch in range(iterations):\n",
    "    t0 = time.time()\n",
    "\n",
    "    xtrue, indicator, inputs = syntrain()\n",
    "    labels = numpy2cuda(indicator if indicatorloss else xtrue, single)\n",
    "\n",
    "    if batchsize is None:\n",
    "      batchsize = inputs.shape[0]\n",
    "    batches = inputs.shape[0] // batchsize\n",
    "\n",
    "    averaged_loss = 0.0    \n",
    "    \n",
    "    for i in range(batches):\n",
    "      # zero the parameter gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # forward + backward + optimize\n",
    "      outputs = net(inputs[i*batchsize:(i+1)*batchsize])\n",
    "      loss = lossfunction(outputs, labels[i*batchsize:(i+1)*batchsize])\n",
    "      loss.backward()\n",
    "      \n",
    "      if clipgradient is not None:\n",
    "        torch.nn.utils.clip_grad_norm_(net.parameters(), clipgradient)\n",
    "      \n",
    "      optimizer.step()\n",
    "\n",
    "      # print statistics\n",
    "      averaged_loss += loss.item()\n",
    "\n",
    "    training_loss.append(averaged_loss/batches)\n",
    "\n",
    "    if validation is not None:\n",
    "      loss = lossfunction(net(vinputs), vlabels)\n",
    "      validation_loss.append(loss.detach().cpu().item())\n",
    "\n",
    "    if epoch == 1:\n",
    "      print(\"One epoch = {:.1f} seconds.\".format(time.time() - t0))\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "      print(epoch,training_loss[-1],validation_loss[-1] if validation is not None else '')\n",
    "\n",
    "    try:\n",
    "      if len(training_loss) > iterations/10:\n",
    "        training_rate = np.polyfit(range(20), training_loss[-20:], deg=1)[0]\n",
    "        if training_rate < 0 and training_rate > -finalv:\n",
    "          print(f\"Terminating at epoch {epoch} because training loss stopped improving sufficiently: rate = {training_rate}\")\n",
    "          break\n",
    "\n",
    "      if len(validation_loss) > iterations/10:\n",
    "        validation_rate = np.polyfit(range(20), validation_loss[-20:], deg=1)[0]        \n",
    "        if validation_rate > 0:\n",
    "          print(f\"Terminating at epoch {epoch} because validation loss started worsening: rate = {validation_rate}\")\n",
    "          break\n",
    "    except:\n",
    "      pass\n",
    "          \n",
    "  print(\"Final\",training_loss[-1],validation_loss[-1] if validation is not None else '')\n",
    "      \n",
    "  if hasattr(net,'steps'):\n",
    "    net.steps += iterations\n",
    "  else:\n",
    "    net.steps = iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [200*2] + [1024]*8 + [1*6]\n",
    "percival_network = makenet(dimensions, softmax=False)\n",
    "\n",
    "network_to_use = percival_network()\n",
    "\n",
    "##Training data to pass through Percival network\n",
    "training_data = lambda: syntrain(size=1000, varx='Mc')\n",
    "\n",
    "##Train Percival network on above data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 109.65727233886719 \n",
      "One epoch = 0.4 seconds.\n",
      "50 -3.1678547859191895 \n",
      "100 -7.168209552764893 \n",
      "150 -10.986236572265625 \n",
      "200 -19.550262451171875 \n",
      "250 -7.834001064300537 \n",
      "300 -9.823049545288086 \n",
      "350 -14.58784008026123 \n",
      "400 -18.5667781829834 \n",
      "450 -21.263141632080078 \n",
      "500 -23.868864059448242 \n",
      "550 -23.86467170715332 \n",
      "600 -26.513507843017578 \n",
      "650 -30.551664352416992 \n",
      "700 -24.40126609802246 \n",
      "750 -12.856674194335938 \n",
      "800 -30.910032272338867 \n",
      "850 -31.389015197753906 \n",
      "900 -38.95603942871094 \n",
      "950 -31.47637176513672 \n",
      "1000 -36.50532150268555 \n",
      "1050 -7.687808513641357 \n",
      "1100 -3.475628614425659 \n",
      "1150 -3.3763017654418945 \n",
      "1200 -3.388878583908081 \n",
      "1250 -3.410987377166748 \n",
      "1300 -3.5615220069885254 \n",
      "1350 -3.5643470287323 \n",
      "1400 -3.6366965770721436 \n",
      "1450 -3.7702858448028564 \n",
      "1500 -3.783756971359253 \n",
      "1550 -3.88922119140625 \n",
      "1600 -4.054741382598877 \n",
      "1650 -3.949733018875122 \n",
      "1700 -4.057702541351318 \n",
      "1750 -4.3162150382995605 \n",
      "1800 -4.442042350769043 \n",
      "1850 -4.572079181671143 \n",
      "1900 -4.877509117126465 \n",
      "1950 -5.148087024688721 \n",
      "2000 -5.524396896362305 \n",
      "2050 -5.541443824768066 \n",
      "2100 -5.723745822906494 \n",
      "2150 -6.427175045013428 \n",
      "2200 -7.897294521331787 \n",
      "2250 -9.194194793701172 \n",
      "2300 -6.324650764465332 \n",
      "2350 -6.891244411468506 \n",
      "2400 -8.946174621582031 \n",
      "2450 -6.6056342124938965 \n",
      "2500 -7.507547855377197 \n",
      "2550 -9.28642463684082 \n",
      "2600 -7.715606212615967 \n",
      "2650 -15.239912986755371 \n",
      "2700 -25.292858123779297 \n",
      "2750 -3.670196056365967 \n",
      "2800 -4.371192932128906 \n",
      "2850 -4.630543231964111 \n",
      "2900 -4.835017204284668 \n",
      "2950 -4.9674248695373535 \n",
      "3000 -5.10815954208374 \n",
      "3050 -5.327772617340088 \n",
      "3100 -5.384321212768555 \n",
      "3150 -5.557695388793945 \n",
      "3200 -5.635012149810791 \n",
      "3250 -5.736041069030762 \n",
      "3300 -6.052846908569336 \n",
      "3350 -6.167089939117432 \n",
      "3400 -6.538447380065918 \n",
      "3450 -6.6792707443237305 \n",
      "3500 -7.156112194061279 \n",
      "3550 -7.523340225219727 \n",
      "3600 -8.520691871643066 \n",
      "3650 -11.046731948852539 \n",
      "3700 -4.515263080596924 \n",
      "3750 -5.952310085296631 \n",
      "3800 -6.219981670379639 \n",
      "3850 -6.531627178192139 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-dcb50b795d89>\u001b[0m in \u001b[0;36msyntrainer\u001b[1;34m(net, syntrain, lossfunction, iterations, batchsize, initstep, finalv, clipgradient, validation, seed, single)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mt0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mxtrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindicator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msyntrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy2cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindicator\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mindicatorloss\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mxtrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msingle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-38d4fd00a021>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m##Training data to pass through Percival network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtraining_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0msyntrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvarx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Mc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m##Train Percival network on above data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-12e90f147462>\u001b[0m in \u001b[0;36msyntrain\u001b[1;34m(size, region, snrs, varx, varall, seed, single, noi)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#generating signal and noise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0msignal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh_func_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxs_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0msignal_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignal_i\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy2cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy2cuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\Programming\\lib\\site-packages\\numpy\\lib\\shape_base.py\u001b[0m in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[0mbuff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minds\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0mbuff\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minarr_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "##training the network\n",
    "syntrainer(network_to_use, training_data, lossfunction=lossG1, iterations=5000,\n",
    "           initstep=1e-4, finalv=1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"C:\\Users\\Luke\\year-4-project-lisa\\Luke\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(network_to_use.state_dict(), PATH + '') #insert name for net e.g #percival/Mc-nu_l1024x8_g1_SNR8-16_2d.pt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
